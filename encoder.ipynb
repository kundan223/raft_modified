{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd2a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from quaternion_layers import *\n",
    "from BatchNormalization import QuaternionBatchNorm2d \n",
    "from InstanceNormalization import QuaternionInstanceNorm2d\n",
    "from ReLu import Relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07edb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        num_groups = planes // 8\n",
    "\n",
    "        if norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "\n",
    "        elif norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(planes)\n",
    "            self.norm2 = nn.BatchNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(planes)\n",
    "            self.norm2 = nn.InstanceNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.InstanceNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "            self.norm2 = nn.Sequential()\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.Sequential()\n",
    "\n",
    "        if stride == 1:\n",
    "            self.downsample = None\n",
    "        \n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride), self.norm3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'Input x size: {x.size()}')  # Print input size\n",
    "        y = x\n",
    "        print(\"shape of x in conv1\", y.shape)\n",
    "        y = self.relu(self.norm1(self.conv1(y)))\n",
    "        print(f'After conv1 and norm1 size of y : {y.size()}')  # Print size after first convolution and normalization\n",
    "        \n",
    "        y = self.relu(self.norm2(self.conv2(y)))\n",
    "        print(f'After conv2 and norm2 sizeof y : {y.size()}')  # Print size after second convolution and normalization\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "            print(f'After downsample size of x : {x.size()}')  # Print size after downsampling\n",
    "\n",
    "        output = self.relu(x + y)\n",
    "        print(f'Output size: {output.size()}')  # Print output size\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2e046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: torch.Size([4, 16, 64, 64])\n",
      "Input x size: torch.Size([4, 16, 64, 64])\n",
      "shape of x in conv1 torch.Size([4, 16, 64, 64])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 32, 32, 32])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 32, 32, 32])\n",
      "After downsample size of x : torch.Size([4, 32, 32, 32])\n",
      "Output size: torch.Size([4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ResidualBlock\n",
    "in_channels = 16  # Example input channels\n",
    "out_channels = 32  # Example output channels\n",
    "block = ResidualBlock(in_planes=in_channels, planes=out_channels, norm_fn='group', stride=2)\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, in_channels, height, width)\n",
    "batch_size = 4\n",
    "height, width = 64, 64  # Example spatial dimensions\n",
    "input_tensor = torch.randn(batch_size, in_channels, height, width)\n",
    "\n",
    "\n",
    "print(\"Input dimensions:\", input_tensor.shape)\n",
    "\n",
    "output_tensor = block(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83438d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9d5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n",
    "        super(BasicEncoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n",
    "            \n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(64)\n",
    "\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.layer1 = self._make_layer(64,  stride=1)\n",
    "        self.layer2 = self._make_layer(96, stride=2)\n",
    "        self.layer3 = self._make_layer(128, stride=2)\n",
    "\n",
    "        # output convolution\n",
    "        self.conv2 = nn.Conv2d(128, output_dim, kernel_size=1)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = ResidualBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = ResidualBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "        \n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"input in encoder\", x.shape)\n",
    "\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[0]\n",
    "            x = torch.cat(x, dim=0)\n",
    "            \n",
    "        print(\"input in encoder going through convolution\", x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.training and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if is_list:\n",
    "            x = torch.split(x, [batch_dim, batch_dim], dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca6e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: torch.Size([4, 3, 256, 256])\n",
      "input in encoder torch.Size([4, 3, 256, 256])\n",
      "input in encoder going through convolution torch.Size([4, 3, 256, 256])\n",
      "Input x size: torch.Size([4, 64, 128, 128])\n",
      "shape of x in conv1 torch.Size([4, 64, 128, 128])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 64, 128, 128])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 64, 128, 128])\n",
      "Output size: torch.Size([4, 64, 128, 128])\n",
      "Input x size: torch.Size([4, 64, 128, 128])\n",
      "shape of x in conv1 torch.Size([4, 64, 128, 128])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 64, 128, 128])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 64, 128, 128])\n",
      "Output size: torch.Size([4, 64, 128, 128])\n",
      "Input x size: torch.Size([4, 64, 128, 128])\n",
      "shape of x in conv1 torch.Size([4, 64, 128, 128])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 96, 64, 64])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 96, 64, 64])\n",
      "After downsample size of x : torch.Size([4, 96, 64, 64])\n",
      "Output size: torch.Size([4, 96, 64, 64])\n",
      "Input x size: torch.Size([4, 96, 64, 64])\n",
      "shape of x in conv1 torch.Size([4, 96, 64, 64])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 96, 64, 64])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 96, 64, 64])\n",
      "Output size: torch.Size([4, 96, 64, 64])\n",
      "Input x size: torch.Size([4, 96, 64, 64])\n",
      "shape of x in conv1 torch.Size([4, 96, 64, 64])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 128, 32, 32])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 128, 32, 32])\n",
      "After downsample size of x : torch.Size([4, 128, 32, 32])\n",
      "Output size: torch.Size([4, 128, 32, 32])\n",
      "Input x size: torch.Size([4, 128, 32, 32])\n",
      "shape of x in conv1 torch.Size([4, 128, 32, 32])\n",
      "After conv1 and norm1 size of y : torch.Size([4, 128, 32, 32])\n",
      "After conv2 and norm2 sizeof y : torch.Size([4, 128, 32, 32])\n",
      "Output size: torch.Size([4, 128, 32, 32])\n",
      "Output dimensions: torch.Size([4, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BasicEncoder\n",
    "output_dim = 128  # Example output dimension\n",
    "encoder = BasicEncoder(output_dim=output_dim, norm_fn='batch', dropout=0.1)\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "batch_size = 4\n",
    "input_tensor = torch.randn(batch_size, 3, 256, 256)  # Example input size\n",
    "\n",
    "# Print input dimensions\n",
    "print(\"Input dimensions:\", input_tensor.shape)\n",
    "\n",
    "# Pass the input tensor through the encoder\n",
    "output_tensor = encoder(input_tensor)\n",
    "\n",
    "# Print output dimensions\n",
    "if isinstance(output_tensor, (tuple, list)):\n",
    "    for idx, out in enumerate(output_tensor):\n",
    "        print(f\"Output dimensions [{idx}]:\", out.shape)\n",
    "else:\n",
    "    print(\"Output dimensions:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff155778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n",
    "        super(SmallEncoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=32)\n",
    "            \n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(32)\n",
    "\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.in_planes = 32\n",
    "        self.layer1 = self._make_layer(32,  stride=1)\n",
    "        self.layer2 = self._make_layer(64, stride=2)\n",
    "        self.layer3 = self._make_layer(96, stride=2)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, output_dim, kernel_size=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = BottleneckBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = BottleneckBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "    \n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[0]\n",
    "            x = torch.cat(x, dim=0)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.training and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if is_list:\n",
    "            x = torch.split(x, [batch_dim, batch_dim], dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc85f8d",
   "metadata": {},
   "source": [
    "### MODIFIED ENCODER \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98e57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1(nn.Module):\n",
    "    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n",
    "        super(ResidualBlock1, self).__init__()\n",
    "  \n",
    "        self.conv1 = QuaternionConv(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = QuaternionConv(planes, planes, kernel_size=3, padding=1 , stride =1)\n",
    "        self.relu = Relu()\n",
    "\n",
    "        num_groups = planes // 8\n",
    "\n",
    "        if norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "        \n",
    "        elif norm_fn == 'batch':\n",
    "            self.norm1 = QuaternionBatchNorm2d(planes)\n",
    "            self.norm2 = QuaternionBatchNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = QuaternionBatchNorm2d(planes)\n",
    "        \n",
    "        elif norm_fn == 'instance':\n",
    "            self.norm1 = QuaternionInstanceNorm2d(planes)\n",
    "            self.norm2 = QuaternionInstanceNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = QuaternionInstanceNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "            self.norm2 = nn.Sequential()\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.Sequential()\n",
    "\n",
    "        if stride == 1:\n",
    "            self.downsample = None\n",
    "        \n",
    "        else:    \n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride), self.norm3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'Input x size: {x.size()}')  # Print input size\n",
    "        y = x\n",
    "        print(\"shape of x in conv1\", y.shape)\n",
    "        y = self.relu(self.norm1(self.conv1(y)))\n",
    "        print(f'After conv1 and norm1 size: {y.size()}')  # Print size after first convolution and normalization\n",
    "        y = self.relu(self.norm2(self.conv2(y)))\n",
    "        print(f'After conv2 and norm2 size: {y.size()}')  # Print size after second convolution and normalization\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "            print(f'After downsample size: {x.size()}')  # Print size after downsampling\n",
    "\n",
    "        return self.relu(x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb397911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b3471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: torch.Size([4, 16, 64, 64])\n",
      "Input x size: torch.Size([4, 16, 64, 64])\n",
      "shape of x in conv1 torch.Size([4, 16, 64, 64])\n",
      "After conv1 and norm1 size: torch.Size([4, 32, 32, 32])\n",
      "After conv2 and norm2 size: torch.Size([4, 32, 32, 32])\n",
      "After downsample size: torch.Size([4, 32, 32, 32])\n",
      "Output dimensions: torch.Size([4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ResidualBlock\n",
    "in_channels = 16  # Example input channels\n",
    "out_channels = 32  # Example output channels\n",
    "block = ResidualBlock1(in_planes=in_channels, planes=out_channels, norm_fn='batch', stride=2)\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, in_channels, height, width)\n",
    "batch_size = 4\n",
    "height, width = 64, 64  # Example spatial dimensions\n",
    "input_tensor = torch.randn(batch_size, in_channels, height, width)\n",
    "\n",
    "# Print input dimensions\n",
    "print(\"Input dimensions:\", input_tensor.shape)\n",
    "\n",
    "# Pass the input tensor through the block\n",
    "output_tensor = block(input_tensor)\n",
    "\n",
    "# Print output dimensions\n",
    "print(\"Output dimensions:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338faf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n",
    "        super(BasicEncoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n",
    "            \n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = QuaternionBatchNorm2d(64)\n",
    "\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = QuaternionInstanceNorm2d(64)\n",
    "\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "\n",
    "        self.conv1 = QuaternionConv(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.layer1 = self._make_layer(64,  stride=1)\n",
    "        self.layer2 = self._make_layer(96, stride=2)\n",
    "        self.layer3 = self._make_layer(128, stride=2)\n",
    "\n",
    "        # output convolution\n",
    "        self.conv2 = QuaternionConv(128, output_dim, kernel_size=1)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = ResidualBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = ResidualBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "        \n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"this is the input in encoder\", x.shape)\n",
    "\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[0]\n",
    "            x = torch.cat(x, dim=0)\n",
    "            \n",
    "        \n",
    "        print(\"Input shape:\", x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.training and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if is_list:\n",
    "            x = torch.split(x, [batch_dim, batch_dim], dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932b508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: torch.Size([4, 3, 256, 256])\n",
      "this is the input in encoder torch.Size([4, 3, 256, 256])\n",
      "Input shape: torch.Size([4, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 0, 7, 7], expected input[4, 3, 256, 256] to have 0 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput dimensions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Pass the input tensor through the encoder\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m encoder(input_tensor)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Print output dimensions\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_tensor, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 63\u001b[0m, in \u001b[0;36mBasicEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\RAFT\\RAFT\\core\\quaternion_layers.py:164\u001b[0m, in \u001b[0;36mQuaternionConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quaternion_conv_rotation(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_kernel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mj_weight,\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilatation,\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquaternion_format, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_param)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quaternion_conv(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mj_weight,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilatation)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\RAFT\\RAFT\\core\\quaternion_ops.py:175\u001b[0m, in \u001b[0;36mquaternion_conv\u001b[1;34m(input, r_weight, i_weight, j_weight, k_weight, bias, stride, padding, groups, dilatation)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe convolutional input is either 3, 4 or 5 dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m input.dim = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convfunc(\u001b[38;5;28minput\u001b[39m, cat_kernels_4_quaternion, bias, stride, padding, dilatation, groups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 0, 7, 7], expected input[4, 3, 256, 256] to have 0 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "# Initialize the BasicEncoder\n",
    "output_dim = 128  # Example output dimension\n",
    "encoder = BasicEncoder(output_dim=output_dim, norm_fn='batch', dropout=0.1)\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "batch_size = 4\n",
    "input_tensor = torch.randn(batch_size, 3, 256, 256)  # Example input size\n",
    "\n",
    "# Print input dimensions\n",
    "print(\"Input dimensions:\", input_tensor.shape)\n",
    "\n",
    "# Pass the input tensor through the encoder\n",
    "output_tensor = encoder(input_tensor)\n",
    "\n",
    "# Print output dimensions\n",
    "if isinstance(output_tensor, (tuple, list)):\n",
    "    for idx, out in enumerate(output_tensor):\n",
    "        print(f\"Output dimensions [{idx}]:\", out.shape)\n",
    "else:\n",
    "    print(\"Output dimensions:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af5c6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "  \n",
    "        self.conv1 = QuaternionConv(in_planes, planes//4, kernel_size=1, padding=0)\n",
    "        self.conv2 = QuaternionConv(planes//4, planes//4, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv3 = QuaternionConv(planes//4, planes, kernel_size=1, padding=0)\n",
    "        self.relu = Relu()\n",
    "\n",
    "        num_groups = planes // 8\n",
    "\n",
    "        if norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes//4)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes//4)\n",
    "            self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            if not stride == 1:\n",
    "                self.norm4 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "        \n",
    "        elif norm_fn == 'batch':\n",
    "            self.norm1 = QuaternionBatchNorm2d(planes//4)\n",
    "            self.norm2 = QuaternionBatchNorm2d(planes//4)\n",
    "            self.norm3 = QuaternionBatchNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm4 = QuaternionBatchNorm2d(planes)\n",
    "        \n",
    "        elif norm_fn == 'instance':\n",
    "            self.norm1 = QuaternionInstanceNorm2d(planes//4)\n",
    "            self.norm2 = QuaternionInstanceNorm2d(planes//4)\n",
    "            self.norm3 = QuaternionInstanceNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm4 = QuaternionInstanceNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "            self.norm2 = nn.Sequential()\n",
    "            self.norm3 = nn.Sequential()\n",
    "            if not stride == 1:\n",
    "                self.norm4 = nn.Sequential()\n",
    "\n",
    "        if stride == 1:\n",
    "            self.downsample = None\n",
    "        \n",
    "        else:    \n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride), self.norm4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.relu(self.norm1(self.conv1(y)))\n",
    "        y = self.relu(self.norm2(self.conv2(y)))\n",
    "        y = self.relu(self.norm3(self.conv3(y)))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return self.relu(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f1b91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n",
    "        super(SmallEncoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=32)\n",
    "            \n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = QuaternionBatchNorm2d(32)\n",
    "\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = QuaternionInstanceNorm2d(32)\n",
    "\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "\n",
    "        self.conv1 = QuaternionConv(3, 32, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.in_planes = 32\n",
    "        self.layer1 = self._make_layer(32,  stride=1)\n",
    "        self.layer2 = self._make_layer(64, stride=2)\n",
    "        self.layer3 = self._make_layer(96, stride=2)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "        \n",
    "        self.conv2 = QuaternionConv(96, output_dim, kernel_size=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuaternionConv):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (QuaternionBatchNorm2d, QuaternionInstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = BottleneckBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = BottleneckBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "    \n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[0]\n",
    "            x = torch.cat(x, dim=0)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.training and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if is_list:\n",
    "            x = torch.split(x, [batch_dim, batch_dim], dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d3a5d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QuaternionBatchNorm2d' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the BasicEncoder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m  \u001b[38;5;66;03m# Example output dimension\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m SmallEncoder(output_dim\u001b[38;5;241m=\u001b[39moutput_dim, norm_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a dummy input tensor with shape (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "Cell \u001b[1;32mIn[18], line 36\u001b[0m, in \u001b[0;36mSmallEncoder.__init__\u001b[1;34m(self, output_dim, norm_fn, dropout)\u001b[0m\n\u001b[0;32m     34\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_normal_(m\u001b[38;5;241m.\u001b[39mweight, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfan_out\u001b[39m\u001b[38;5;124m'\u001b[39m, nonlinearity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, (QuaternionBatchNorm2d, QuaternionInstanceNorm2d, nn\u001b[38;5;241m.\u001b[39mGroupNorm)):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QuaternionBatchNorm2d' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# Initialize the BasicEncoder\n",
    "output_dim = 128  # Example output dimension\n",
    "encoder = SmallEncoder(output_dim=output_dim, norm_fn='batch', dropout=0.1)\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, channels, height, width)\n",
    "batch_size = 4\n",
    "input_tensor = torch.randn(batch_size, 3, 256, 256)  # Example input size\n",
    "\n",
    "# Print input dimensions\n",
    "print(\"Input dimensions:\", input_tensor.shape)\n",
    "\n",
    "# Pass the input tensor through the encoder\n",
    "output_tensor = encoder(input_tensor)\n",
    "\n",
    "# Print output dimensions\n",
    "if isinstance(output_tensor, (tuple, list)):\n",
    "    for idx, out in enumerate(output_tensor):\n",
    "        print(f\"Output dimensions [{idx}]:\", out.shape)\n",
    "else:\n",
    "    print(\"Output dimensions:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c2d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
